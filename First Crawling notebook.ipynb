{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9618d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c58a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06388bbc",
   "metadata": {},
   "source": [
    "## First part - get Data: Basic functions that will be used \n",
    "Functions include:\n",
    "getPagesToCrawl - a function that creates a list of pages that should be used in order to achieve the full data.\n",
    "getTableDataFromPage - from a single page we get back a table that stores the data\n",
    "cleanHtmlCode - the first step that we take in order to clean our data, we remove the HTML from it and leave the info.\n",
    "createDf - gets 8 lists that contains data and return a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c7f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPagesToCrawl():\n",
    "    baseUrl=\"https://www.start.umd.edu/gtd/search/Results.aspx?page=\"\n",
    "    amountPages = \"&count=2000\" \n",
    "    pageList = []\n",
    "    \n",
    "    for currentPageNumber in range(1,102):\n",
    "        pageList.append(baseUrl + str(currentPageNumber) +amountPages)\n",
    "        \n",
    "    return pageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f59b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableDataFromPage(pageUrl):\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(pageUrl,headers=user_agent)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    table = soup.find('table')\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeffd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtmlCode(tableData):\n",
    "    GTD_ID = []\n",
    "    DATE = []\n",
    "    COUNTRY = []\n",
    "    CITY = [] \n",
    "    PERPETRATOR_GROUP = []\n",
    "    FATALITIES = []\n",
    "    INJURED = []\n",
    "    TARGET_TYPE = []\n",
    "\n",
    "    listRows = []\n",
    "    for tr in tableData.findAll(\"tr\",attrs={}):\n",
    "        for td in tr:\n",
    "            if(td != \"\\n\"):\n",
    "                listRows.append(td.text)\n",
    "\n",
    "\n",
    "    # A lot of data that we don't need (The name of the columns (8)) \n",
    "    for i in range(8):\n",
    "        listRows.pop(0)\n",
    "    \n",
    "    for i in range(len(listRows)):\n",
    "        iMod = i%8\n",
    "        if(iMod == 0):\n",
    "            GTD_ID.append(listRows[i])\n",
    "        elif(iMod == 1):\n",
    "            DATE.append(listRows[i])\n",
    "        elif(iMod == 2):\n",
    "            COUNTRY.append(listRows[i])\n",
    "        elif(iMod == 3):\n",
    "            CITY.append(listRows[i])\n",
    "        elif(iMod == 4):\n",
    "            PERPETRATOR_GROUP.append(listRows[i])\n",
    "        elif(iMod == 5):\n",
    "            FATALITIES.append(listRows[i])\n",
    "        elif(iMod == 6):\n",
    "            INJURED.append(listRows[i])\n",
    "        elif(iMod == 7):\n",
    "            TARGET_TYPE.append(listRows[i])\n",
    "       \n",
    "    \n",
    "    return GTD_ID,DATE,COUNTRY,CITY,PERPETRATOR_GROUP,FATALITIES,INJURED,TARGET_TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b4eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDf(GTD_ID,DATE,COUNTRY,CITY,PERPETRATOR_GROUP,FATALITIES,INJURED,TARGET_TYPE):\n",
    "    df = pd.DataFrame({\"GTD_ID\":GTD_ID,\n",
    "                   \"DATE\":DATE,\n",
    "                   \"COUNTRY\":COUNTRY,\n",
    "                   \"CITY\":CITY,\n",
    "                   \"PERPETRATOR_GROUP\":PERPETRATOR_GROUP,\n",
    "                   \"FATALITIES\":FATALITIES\n",
    "                   ,\"INJURED\":INJURED,\n",
    "                   \"TARGET_TYPE\":TARGET_TYPE,\n",
    "                   })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9197766",
   "metadata": {},
   "source": [
    "## First part - get Data: Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fa84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 Lists that will store the data.\n",
    "GTD_ID = []\n",
    "DATE = []\n",
    "COUNTRY = []\n",
    "CITY = [] \n",
    "PERPETRATOR_GROUP = []\n",
    "FATALITIES = []\n",
    "INJURED = []\n",
    "TARGET_TYPE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we Need the URLS which will be used by the crawlling method.\n",
    "pageList = getPagesToCrawl()\n",
    "\n",
    "# For each page inside our pageList:\n",
    "for page in pageList:\n",
    "    \n",
    "    # Get the table data from the page with getTableDataFromPage function.\n",
    "    tableData = getTableDataFromPage(page)\n",
    "    \n",
    "    # Extract 8 lists (according to parameter) with cleanHtmlCode function.\n",
    "    GTD_ID_temp,DATE_temp,COUNTRY_temp,CITY_temp,PERPETRATOR_GROUP_temp,FATALITIES_temp,INJURED_temp,TARGET_TYPE_temp = cleanHtmlCode(tableData)\n",
    "    \n",
    "    # The the temp lists and EXTEND the data to non-temp list.\n",
    "    GTD_ID.extend(GTD_ID_temp)\n",
    "    DATE.extend(DATE_temp)\n",
    "    COUNTRY.extend(COUNTRY_temp)\n",
    "    CITY.extend(CITY_temp)\n",
    "    PERPETRATOR_GROUP.extend(PERPETRATOR_GROUP_temp)\n",
    "    FATALITIES.extend(FATALITIES_temp)\n",
    "    INJURED.extend(INJURED_temp)\n",
    "    TARGET_TYPE.extend(TARGET_TYPE_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the crawlling method is done all we have left to do is create a DF with createDf function and save the df to csv file.\n",
    "df = createDf(GTD_ID,DATE,COUNTRY,CITY,PERPETRATOR_GROUP,FATALITIES,INJURED,TARGET_TYPE) \n",
    "df.to_csv('GTD_Data_Frame.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the df once again to see that everything is ok\n",
    "df = pd.read_csv('GTD_Data_Frame.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['GTD_ID'], keep='first')\n",
    "df.to_csv('GTD_Data_Frame_Without_Duplicates.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39862f93",
   "metadata": {},
   "source": [
    "#### Second part of crawlling will be in a different notebook (for convinence) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
